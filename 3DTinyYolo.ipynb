{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Packages"
      ],
      "metadata": {
        "id": "0Mg2KET6Nd1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmLdVQ4xNdLF"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dropout, Reshape, Activation, Conv3D, Input, MaxPooling3D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.layers.merge import concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, cv2\n",
        "from preprocessing3D import parse_annotation, BatchGenerator\n",
        "\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 8.0 \n",
        "set_session(tf.Session(config=config))\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"TF_CUDA_HOST_MEM_LIMIT_IN_MB\"] = \"120000\"\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"TRUE\"\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Parameters"
      ],
      "metadata": {
        "id": "mPpjubswNrG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the parameters according to your dataset!"
      ],
      "metadata": {
        "id": "6UFO5pJdOu08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = ['NGpair']\n",
        "\n",
        "IMAGE_Z, IMAGE_H, IMAGE_W = 64, 416, 416    # when changing the 96 change as well in preproceessing3d and utils3d\n",
        "GRID_Z,  GRID_H, GRID_W   = 2, 13, 13\n",
        "BOX              = 5\n",
        "CLASS            = len(LABELS)\n",
        "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
        "OBJ_THRESHOLD    = 0.5 #0.5\n",
        "NMS_THRESHOLD    = 0.45 #0.45\n",
        "ANCHORS          = [0.56,0.61, 0.05, 0.71,1.17, 0.06, 1.07,1.50, 0.1, 1.08,0.82, 0.08, 1.65,1.05, 0.1]\n",
        "#ANCHORS          = [0.4, 0.4, 0.4,    0.6, 0.6, 0.6]\n",
        "\n",
        "NO_OBJECT_SCALE  = 1.0\n",
        "OBJECT_SCALE     = 5.0\n",
        "COORD_SCALE      = 1.0\n",
        "CLASS_SCALE      = 1.0\n",
        "\n",
        "BATCH_SIZE       = 1  # 16\n",
        "WARM_UP_BATCHES  = 0 \n",
        "TRUE_BOX_BUFFER  = 50"
      ],
      "metadata": {
        "id": "5eyxpO8XNnm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Training and Validation Directories"
      ],
      "metadata": {
        "id": "n7DM_DoQNwOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the training and validation directories!"
      ],
      "metadata": {
        "id": "_W91Ink-Oyyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_folder = '/dev/shm/datasetyolo/train/images/'\n",
        "train_annot_folder = '/dev/shm/datasetyolo/train/annot/'\n",
        "valid_image_folder = '/dev/shm/datasetyolo/val/images/'\n",
        "valid_annot_folder = '/dev/shm/datasetyolo/val/annot/'"
      ],
      "metadata": {
        "id": "ExRCwHlHNvil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct the Network"
      ],
      "metadata": {
        "id": "_I8gqG2gN5Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = Input(shape=(IMAGE_Z, IMAGE_H, IMAGE_W, 3))\n",
        "true_boxes  = Input(shape=(1, 1, 1, 1, TRUE_BOX_BUFFER , 6))  # ?,?,?,  #, x,y,z,h,w,d \n",
        "dropout_rate = 0.0\n",
        "\n",
        "# Layer 1\n",
        "x = Conv3D(16, (3,3,3), strides=(1,1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
        "x = BatchNormalization(name='norm_1')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "\n",
        "# Layer 2 - 5\n",
        "for i in range(0,4):\n",
        "    x = Conv3D(32*(2**i), (3,3,3), strides=(1,1,1), padding='same', name='conv_' + str(i+2), use_bias=False)(x)\n",
        "    x = BatchNormalization(name='norm_' + str(i+2))(x)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "\n",
        "# Layer 6\n",
        "x = Conv3D(512, (3,3, 3), strides=(1,1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_6')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling3D(pool_size=(2, 2,2), strides=(1,1,1), padding='same')(x)\n",
        "\n",
        "# Layer 7 - 8\n",
        "for i in range(0,2):\n",
        "    x = Conv3D(1024, (3,3,3), strides=(1,1,1), padding='same', name='conv_' + str(i+7), use_bias=False)(x)\n",
        "    x = BatchNormalization(name='norm_' + str(i+7))(x)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# make the object detection layer\n",
        "output = Conv3D(BOX * (6 + 1 + CLASS), \n",
        "                (1,1,1), strides=(1,1,1), \n",
        "                padding='same', \n",
        "                name='DetectionLayer', \n",
        "                kernel_initializer='lecun_normal')(x)\n",
        "output = Reshape((GRID_Z, GRID_H, GRID_W, BOX, 6 + 1 + CLASS))(output)\n",
        "\n",
        "# small hack to allow true_boxes to be registered when Keras build the model \n",
        "# for more information: https://github.com/fchollet/keras/issues/2790\n",
        "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
        "\n",
        "model = Model([input_image, true_boxes], output)\n",
        "\n",
        "model.summary(positions=[0.2,0.5,0.6,0.8,1.0])"
      ],
      "metadata": {
        "id": "ysU70I06N3NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Loss Function"
      ],
      "metadata": {
        "id": "SMOagGO7OClN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    mask_shape = tf.shape(y_true)[:5]\n",
        "#    mask_shape = tf.shape(y_true)[:4]\n",
        "\n",
        "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H*GRID_Z]),(1, GRID_Z, GRID_H, GRID_W, 1, 1)))\n",
        "    cell_y = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_H), [GRID_W*GRID_Z]),(1, GRID_Z, GRID_W, GRID_H, 1, 1)))\n",
        "    cell_y = tf.transpose(cell_y,(0,1,3,2,4,5))\n",
        "    cell_z = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_Z), [GRID_H*GRID_W]),(1, GRID_W, GRID_H, GRID_Z, 1, 1)))\n",
        "    cell_z = tf.transpose(cell_z,(0,3,2,1,4,5))\n",
        "    \n",
        "#    cell_y = tf.transpose(cell_x, (0,2,1,3,4,5))\n",
        "#    cell_z = tf.transpose(cell_x, (0,3,2,1,4,5))\n",
        "\n",
        "    cell_grid = tf.tile(tf.concat([cell_x,cell_y,cell_z], -1), [BATCH_SIZE, 1, 1, 1, BOX , 1])\n",
        " #   cell_grid = tf.tile(tf.concat([cell_x,cell_y,cell_z], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
        "    \n",
        "    coord_mask = tf.zeros(mask_shape)\n",
        "    conf_mask  = tf.zeros(mask_shape)\n",
        "    class_mask = tf.zeros(mask_shape)\n",
        "    \n",
        "    seen = tf.Variable(0.)\n",
        "    total_recall = tf.Variable(0.)\n",
        "    \n",
        "    \"\"\"\n",
        "    Adjust prediction\n",
        "    \"\"\"\n",
        "    ### adjust x and y      \n",
        "    pred_box_xy = tf.sigmoid(y_pred[..., :3]) + cell_grid\n",
        "    \n",
        "    ### adjust w and h\n",
        "    pred_box_wh = tf.exp(y_pred[..., 3:6]) * np.reshape(ANCHORS, [1,1,1,1,BOX,3])\n",
        "    ### adjust confidence\n",
        "    pred_box_conf = tf.sigmoid(y_pred[..., 6])\n",
        "    \n",
        "    ### adjust class probabilities\n",
        "    pred_box_class = y_pred[..., 7:]\n",
        "    \n",
        "    \"\"\"\n",
        "    Adjust ground truth\n",
        "    \"\"\"\n",
        "    ### adjust x and y\n",
        "    true_box_xy = y_true[..., 0:3] # relative position to the containing cell\n",
        "   \n",
        "    ### adjust w and h\n",
        "    true_box_wh = y_true[..., 3:6] # number of cells accross, horizontally and vertically\n",
        "    \n",
        "    ### adjust confidence\n",
        "    true_wh_half = true_box_wh / 2.\n",
        "    true_mins    = true_box_xy - true_wh_half\n",
        "    true_maxes   = true_box_xy + true_wh_half\n",
        "    \n",
        "    pred_wh_half = pred_box_wh / 2.\n",
        "    pred_mins    = pred_box_xy - pred_wh_half\n",
        "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
        "    \n",
        "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1] * intersect_wh[..., 2]\n",
        "    \n",
        "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1] * true_box_wh[..., 2] \n",
        "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1] * pred_box_wh[..., 2] \n",
        "#    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1] \n",
        "#    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "    user5 = iou_scores[0,0,6,4,...]\n",
        "    user6 = y_pred[0,0,6,4,...,6]\n",
        "    true_box_conf = iou_scores * y_true[..., 6]\n",
        "\n",
        "    ### adjust class probabilities\n",
        "    true_box_class = tf.to_int64(0 *  y_true[..., 6])   # was int32\n",
        "#    true_box_class = tf.argmax(y_true[..., 7:], -1)     # original: get index of maximal value over all classes\n",
        "    \n",
        "    \"\"\"\n",
        "    Determine the masks\n",
        "    \"\"\"\n",
        "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
        "    coord_mask = tf.expand_dims(y_true[..., 6], axis=-1) * COORD_SCALE\n",
        "    \n",
        "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
        "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
        "    true_xy = true_boxes[..., 0:3]\n",
        "    true_wh = true_boxes[..., 3:6]\n",
        "    \n",
        "    true_wh_half = true_wh / 2.\n",
        "    true_mins    = true_xy - true_wh_half\n",
        "    true_maxes   = true_xy + true_wh_half\n",
        "    \n",
        "    pred_xy = tf.expand_dims(pred_box_xy, 5)  \n",
        "    pred_wh = tf.expand_dims(pred_box_wh, 5)  \n",
        "    \n",
        "    pred_wh_half = pred_wh / 2.\n",
        "    pred_mins    = pred_xy - pred_wh_half\n",
        "    pred_maxes   = pred_xy + pred_wh_half    \n",
        "    \n",
        "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1] * intersect_wh[..., 2] \n",
        "    \n",
        "    true_areas = true_wh[..., 0] * true_wh[..., 1] * true_wh[..., 2]\n",
        "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1] * pred_wh[..., 2]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "\n",
        "    best_ious = tf.reduce_max(iou_scores, axis=5) \n",
        "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 6]) * NO_OBJECT_SCALE          ###### was best_ious < 0.6     -------\n",
        "    \n",
        "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
        "    conf_mask = conf_mask + y_true[..., 6] * OBJECT_SCALE\n",
        "    \n",
        "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
        "    class_mask = y_true[..., 6] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE       \n",
        "    \n",
        "    \"\"\"\n",
        "    Warm-up training\n",
        "    \"\"\"\n",
        "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
        "    seen = tf.assign_add(seen, 1.)\n",
        "\n",
        "    \n",
        "\n",
        "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
        "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
        "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,1,BOX,3]) * no_boxes_mask, \n",
        "                                   tf.ones_like(coord_mask)],\n",
        "                          lambda: [true_box_xy, \n",
        "                                   true_box_wh,\n",
        "                                   coord_mask])\n",
        "   \n",
        "    \"\"\"\n",
        "    Finalize the loss\n",
        "    \"\"\"\n",
        "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
        "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
        "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
        "    \n",
        "\n",
        "\n",
        "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_wh_pred    = tf.reduce_sum(pred_box_wh     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_wh_true    = tf.reduce_sum(true_box_wh     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
        "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
        "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
        "    \n",
        "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
        "    \n",
        "    nb_true_box = tf.reduce_sum(y_true[..., 6])\n",
        "    user1 = y_true[0,0,6,4,...,2]\n",
        "    user2 = pred_box_xy[0,0,6,4,...,2]\n",
        "    user3 = nb_coord_box\n",
        "    user4 = nb_conf_box   \n",
        "\n",
        "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
        "\n",
        "    \"\"\"\n",
        "    Debugging code\n",
        "    \"\"\"   \n",
        "#    sess = K.get_session()\n",
        "#    sess.run(tf.contrib.memory_stats.BytesInUse())\n",
        "    user7 = tf.contrib.memory_stats.BytesInUse()\n",
        "\n",
        "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
        "    total_recall = tf.assign_add(total_recall, current_recall) \n",
        "\n",
        "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_wh_pred], message='Loss WH pred\\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_wh_true], message='Loss WH true\\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "yDoCjvULN990"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Parameters"
      ],
      "metadata": {
        "id": "uCqaxhs8OQZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_config = {\n",
        "    'IMAGE_Z'         : IMAGE_Z, \n",
        "    'IMAGE_H'         : IMAGE_H,\n",
        "    'IMAGE_W'         : IMAGE_W,\n",
        "    'GRID_Z'          : GRID_Z,  \n",
        "    'GRID_H'          : GRID_H,\n",
        "    'GRID_W'          : GRID_W,\n",
        "    'BOX'             : BOX,\n",
        "    'LABELS'          : LABELS,\n",
        "    'CLASS'           : len(LABELS),\n",
        "    'ANCHORS'         : ANCHORS,\n",
        "    'BATCH_SIZE'      : BATCH_SIZE,\n",
        "    'TRUE_BOX_BUFFER' : 50,\n",
        "}\n",
        "\n",
        "def normalize(image):\n",
        "    return image / 255.\n",
        "\n",
        "train_imgs, seen_train_labels = parse_annotation(train_annot_folder, train_image_folder, labels=LABELS)\n",
        "train_batch = BatchGenerator(train_imgs, generator_config, norm=normalize, jitter=False,shuffle=True)\n",
        "\n",
        "valid_imgs, seen_valid_labels = parse_annotation(valid_annot_folder, valid_image_folder, labels=LABELS)\n",
        "valid_batch = BatchGenerator(valid_imgs, generator_config, norm=normalize, jitter=False,shuffle=False)\n",
        "\n",
        "#Setup a few callbacks and start the training\n",
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           min_delta=0.001, \n",
        "                           patience=10000000, \n",
        "                           mode='min', \n",
        "                           verbose=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint('weights_NGPAIRS_3D.h5', \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='min', \n",
        "                             period=1)\n",
        "\n",
        "\n",
        "tb_counter  = len([log for log in os.listdir(os.path.expanduser('/logs/')) if 'ngpair_' in log]) + 1\n",
        "tensorboard = TensorBoard(log_dir=os.path.expanduser('/logs/') + 'ngpair_' + '_' + str(tb_counter), \n",
        "                          histogram_freq=0, \n",
        "                          write_graph=False, \n",
        "                          write_images=False)\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=1.0e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss=custom_loss, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "LmZitaluOI9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Training"
      ],
      "metadata": {
        "id": "BKrO54V2OFtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the number of epochs, and uncomment the first lines if you want to use a pre-trained model."
      ],
      "metadata": {
        "id": "0rbFDqjlO8Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#uncomment these lines after training the first model\n",
        "#pretrained_weights = load_model('weights3D'+ str(tb_counter-1) +'.h5', custom_objects={'custom_loss': custom_loss, 'tf': tf})\n",
        "#pretrained_weights = pretrained_weights.get_weights()\n",
        "#model.set_weights(pretrained_weights)\n",
        "\n",
        "\n",
        "model.fit_generator(generator        = train_batch, \n",
        "                    steps_per_epoch  = len(train_batch), \n",
        "                    epochs           = 100, \n",
        "                    verbose          = 1,\n",
        "                    validation_data  = valid_batch,\n",
        "                    validation_steps = len(valid_batch),\n",
        "                    callbacks        = [early_stop, checkpoint, tensorboard], \n",
        "                    max_queue_size   = 3)\n",
        "\n",
        "\n",
        "model.save('weights3D' + str(tb_counter) + '.h5')   "
      ],
      "metadata": {
        "id": "7Vjg085bOhU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}